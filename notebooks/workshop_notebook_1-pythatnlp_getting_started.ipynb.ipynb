{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --user --upgrade --pre pythainlp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Notebook 1: Getting started with PyThaiNLP üòÜ\n",
    "\n",
    "\n",
    "Updated: 31 October 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, List\n",
    "from functools import reduce\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 1. Word Tokenization\n",
    "\n",
    "Word Tokenization is a process to determin word boundaries in a text or sentence.\n",
    "\n",
    "\n",
    "Given a sentence, the tokenizer then read the sentence and return a list of words (i.e. tokens).\n",
    "\n",
    "```python\n",
    "\n",
    "    definition: Tokenizer(str) -> List[str]\n",
    "    \n",
    "    \n",
    "    \n",
    "    Tokenizer(str:\"‡πÄ‡∏ò‡∏≠‡∏Ñ‡∏∑‡∏≠ My Ambulance ‡∏Ç‡∏≠‡∏á‡∏â‡∏±‡∏ô\")  -> List[\"‡πÄ‡∏ò‡∏≠\", \"‡∏Ñ‡∏∑‡∏≠\", \"My\", \"Ambulance\", \"‡∏Ç‡∏≠‡∏á\", \"‡∏â‡∏±‡∏ô\"]\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Dictionary-based tokenizer\n",
    "\n",
    "\n",
    "Dictionary-based tokenizer is an alogirithm the read through the sentence character by character.  If it found sequences of characters match with a vocabulary in the pre-defined dictionary, it maps sequences of characters as a token.\n",
    "https://www.cs.ait.ac.th/~mdailey/papers/Choochart-Wordseg.pdf\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "dictionary = Set[\"‡∏â‡∏±‡∏ô\", \"‡∏ä‡∏≠‡∏ö\", \"‡∏£‡∏ñ‡πÑ‡∏ü\", \"‡∏£‡∏ñ\", \"‡∏£‡∏î\", \"‡∏ô‡πà‡∏≥\", \"‡∏ï‡πâ‡∏ô\", \"‡πÑ‡∏°‡πâ\", \"‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ\", \" \"]\n",
    "\n",
    "\n",
    "Dictionary_Tokenizer(dictionary:Set[str])\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Longest matching (LM)\n",
    "\n",
    "Longest matching is an algorithm to split words from a sentence by considering logest vocab first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = set([\"‡∏â‡∏±‡∏ô\", \"‡∏ä‡∏≠‡∏ö\", \"‡∏£‡∏ñ‡πÑ‡∏ü\", \"‡∏£‡∏ñ\", \"‡∏£‡∏î\", \"‡∏ô‡πà‡∏≥\", \"‡∏ï‡πâ‡∏ô\", \"‡πÑ‡∏°‡πâ\", \"‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ\", \" \", \"‡∏ü‡πâ‡∏≤\"])\n",
    "\n",
    "def search_longest(term, dictionary):\n",
    "    term_length = len(term)\n",
    "    max_length = 0\n",
    "    for vocab in dictionary:\n",
    "        if term in vocab:\n",
    "            max_length = max(max_length, len(vocab))\n",
    "\n",
    "    return max_length == term_length\n",
    "\n",
    "def Dictionary_Tokenizer_LM_debug(sentence:str, dictionary: Set[str]):\n",
    "    buffer = \"\"\n",
    "    tokens = []\n",
    "    for char in sentence:\n",
    "        buffer += char\n",
    "        print(\"buffer\", buffer)\n",
    "        if search_longest(buffer, dictionary) == True:\n",
    "            print(\"select this token: {}\".format(buffer))\n",
    "            tokens.append(buffer)\n",
    "            buffer = \"\"\n",
    "            print(\"clear the buffer.\")\n",
    "            print(\"\")\n",
    "    return tokens\n",
    "\n",
    "def Dictionary_Tokenizer_LM(sentence:str, dictionary: Set[str]):\n",
    "    buffer = \"\"\n",
    "    tokens = []\n",
    "    for char in sentence:\n",
    "        buffer += char\n",
    "        if search_longest(buffer, dictionary) == True:\n",
    "            tokens.append(buffer)\n",
    "            buffer = \"\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer ‡∏â\n",
      "buffer ‡∏â‡∏±\n",
      "buffer ‡∏â‡∏±‡∏ô\n",
      "select this token: ‡∏â‡∏±‡∏ô\n",
      "clear the buffer.\n",
      "\n",
      "buffer ‡∏ä\n",
      "buffer ‡∏ä‡∏≠\n",
      "buffer ‡∏ä‡∏≠‡∏ö\n",
      "select this token: ‡∏ä‡∏≠‡∏ö\n",
      "clear the buffer.\n",
      "\n",
      "buffer  \n",
      "select this token:  \n",
      "clear the buffer.\n",
      "\n",
      "buffer ‡∏£\n",
      "buffer ‡∏£‡∏ñ\n",
      "buffer ‡∏£‡∏ñ‡πÑ\n",
      "buffer ‡∏£‡∏ñ‡πÑ‡∏ü\n",
      "select this token: ‡∏£‡∏ñ‡πÑ‡∏ü\n",
      "clear the buffer.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['‡∏â‡∏±‡∏ô', '‡∏ä‡∏≠‡∏ö', ' ', '‡∏£‡∏ñ‡πÑ‡∏ü']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dictionary_Tokenizer_LM_debug(\"‡∏â‡∏±‡∏ô‡∏ä‡∏≠‡∏ö ‡∏£‡∏ñ‡πÑ‡∏ü\", dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Question 1:__ Create your own dictionary to tokenize the following sentences that can tokenize all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Å‡∏≤‡∏ï‡∏≤‡∏£‡πå ‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô Qatar Information Technology Exhibition and Conference (QITCOM 2019)\",\n",
    "    \"‡∏ì ‡∏Å‡∏£‡∏∏‡∏á‡πÇ‡∏î‡∏Æ‡∏≤ ‡∏£‡∏±‡∏ê‡∏Å‡∏≤‡∏ï‡∏≤‡∏£‡πå\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the vocabulary to dictionary_lm\n",
    "\n",
    "dictionary_lm = set([\n",
    "    \" \",\n",
    "    \"Qatar\",\n",
    "    \"Information\",\n",
    "    \"‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£\",\n",
    "    \"‡∏Å‡∏≤‡∏ï‡∏≤‡∏£‡πå\",\n",
    "    # add more vocab\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Dictionary_Tokenizer_LM(dictionary_lm):\n",
    "    \n",
    "    tokens_list = [ Dictionary_Tokenizer_LM(sentence, dictionary_lm) for sentence in test_sentences]\n",
    "    character_count_expect = sum([len(sentence) for sentence in test_sentences])\n",
    "    character_count_actual = 0\n",
    "    for tokens in tokens_list:\n",
    "        character_count_actual += sum(map(lambda token : len(token),tokens))\n",
    "\n",
    "    if(character_count_actual == character_count_expect):\n",
    "        print(\"‚úÖ Test succeed. üòÅ\")\n",
    "        \n",
    "        print(\"\\n tokens_list: \", tokens_list)\n",
    "    else:\n",
    "        print(\"Test failed. üò≠\\n\")\n",
    "        \n",
    "        print(\"test_sentences\", test_sentences)\n",
    "        print(\"tokens_list\", tokens_list)\n",
    "        \n",
    "        print('')\n",
    "        print(\"character_count_actual != character_count_expect\")\n",
    "        print(\"{} != {}\".format(character_count_actual, character_count_expect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test failed. üò≠\n",
      "\n",
      "test_sentences ['‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Å‡∏≤‡∏ï‡∏≤‡∏£‡πå ‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô Qatar Information Technology Exhibition and Conference (QITCOM 2019)', '‡∏ì ‡∏Å‡∏£‡∏∏‡∏á‡πÇ‡∏î‡∏Æ‡∏≤ ‡∏£‡∏±‡∏ê‡∏Å‡∏≤‡∏ï‡∏≤‡∏£‡πå']\n",
      "tokens_list [['‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£', '‡∏Å‡∏≤‡∏ï‡∏≤‡∏£‡πå', ' '], []]\n",
      "\n",
      "character_count_actual != character_count_expect\n",
      "33 != 128\n"
     ]
    }
   ],
   "source": [
    "# Run this block to test the code\n",
    "test_Dictionary_Tokenizer_LM(dictionary_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_lm = set([\n",
    "    \"Qatar\",\n",
    "    \"Information\",\n",
    "    \"‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£\",\n",
    "    \"‡∏Å‡∏≤‡∏ï‡∏≤‡∏£‡πå\",\n",
    "    \"‡∏ì\",\n",
    "    \"‡∏Å‡∏£‡∏∏‡∏á‡πÇ‡∏î‡∏Æ‡∏≤\",\n",
    "    \"‡∏£‡∏±‡∏ê\",\n",
    "    \" \",\n",
    "    \"‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô\",\n",
    "    \"(\",\n",
    "    \")\",\n",
    "    \"QITCOM\", \"2019\",\n",
    "    \"Qatar\",\n",
    "    \"Information\",\n",
    "    \"Technology\",\n",
    "    \"Exhibition\",\n",
    "    \"and\",\n",
    "    \"Conference\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test succeed. üòÅ\n",
      "\n",
      " tokens_list:  [['‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£', '‡∏Å‡∏≤‡∏ï‡∏≤‡∏£‡πå', ' ', '‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô', ' ', 'Qatar', ' ', 'Information', ' ', 'Technology', ' ', 'Exhibition', ' ', 'and', ' ', 'Conference', ' ', '(', 'QITCOM', ' ', '2019', ')'], ['‡∏ì', ' ', '‡∏Å‡∏£‡∏∏‡∏á‡πÇ‡∏î‡∏Æ‡∏≤', ' ', '‡∏£‡∏±‡∏ê', '‡∏Å‡∏≤‡∏ï‡∏≤‡∏£‡πå']]\n"
     ]
    }
   ],
   "source": [
    "test_Dictionary_Tokenizer_LM(dictionary_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Maximal matching (MM)\n",
    "\n",
    "\n",
    "Unlike Longest Matching, Maximal matching is an algorithm to split words from a sentence in which it prefers minumum number of tokens to be splited.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "dictionary = set([\"‡∏£‡∏ñ\", \"‡∏£‡∏ñ‡πÑ‡∏ü\", \"‡∏ü‡πâ‡∏≤\", \"‡πÑ‡∏ü‡∏ü‡πâ‡∏≤\", \"‡πÉ‡∏ï‡πâ‡∏î‡∏¥‡∏ô\"])\n",
    "\n",
    "\n",
    "sentence = \"‡∏£‡∏ñ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡πÉ‡∏ï‡πâ‡∏î‡∏¥‡∏ô\"\n",
    "\n",
    "Possible_segments(sentence) ->\n",
    "[\"‡∏£‡∏ñ‡πÑ‡∏ü\", \"‡∏ü‡πâ‡∏≤\", \"‡πÉ‡∏ï‡πâ‡∏î‡∏¥‡∏ô\"]\n",
    "[\"‡∏£‡∏ñ\", \"‡πÑ‡∏ü‡∏ü‡πâ‡∏≤\", \"‡πÉ‡∏ï‡πâ‡∏î‡∏¥‡∏ô\"]\n",
    "[\"‡∏£‡∏ñ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤\", \"‡πÉ‡∏ï‡πâ‡∏î‡∏¥‡∏ô\"]\n",
    "\n",
    "\n",
    "selected_segment = [\"‡∏£‡∏ñ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤\", \"‡πÉ‡∏ï‡πâ‡∏î‡∏¥‡∏ô\"]\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyThaiNLP's Tokenizer (newmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Å‡∏≤‡∏ï‡∏≤‡∏£‡πå ‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô Qatar Information Technology Exhibition and Conference (QITCOM 2019)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°', '‡πÅ‡∏•‡∏∞', '‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£', '‡∏Å‡∏≤‡∏ï‡∏≤‡∏£‡πå', ' ', '‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô', ' ', 'Qatar', ' ', 'Information', ' ', 'Technology', ' ', 'Exhibition', ' ', 'and', ' ', 'Conference', ' ', '(', 'QITCOM', ' ', '2019', ')']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(test_sentence, engine=\"newmm\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Try out:__\n",
    "\n",
    "2.1 Try adding your own sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡∏â‡∏±‡∏ô', '‡∏≠‡∏¢‡∏∏‡πã', '‡∏ó‡∏µ‡πà', ' ', '‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï‡∏û‡∏±‡∏í‡∏ô‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå']\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "print(word_tokenize(\"‡∏â‡∏±‡∏ô‡∏≠‡∏¢‡∏∏‡πã‡∏ó‡∏µ‡πà ‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï‡∏û‡∏±‡∏í‡∏ô‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå\", engine=\"newmm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ']\n"
     ]
    }
   ],
   "source": [
    "# Enter you own setnence\n",
    "print(word_tokenize(\" \", engine=\"newmm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Try adding your own sentence with misspelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡∏â‡∏±‡∏ô', '‡∏≠‡∏¢‡∏∏‡πã', '‡∏ó‡∏µ‡πà', ' ', '‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô', '‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï', '‡∏û‡∏±‡∏í‡∏ô', '‡∏ö‡∏¢', '‡∏£‡∏¥', '‡∏´‡∏≤‡∏£', '‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå']\n"
     ]
    }
   ],
   "source": [
    "# Example sentence with misspelling words\n",
    "print(word_tokenize(\"‡∏â‡∏±‡∏ô‡∏≠‡∏¢‡∏∏‡πã‡∏ó‡∏µ‡πà ‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï‡∏û‡∏±‡∏í‡∏ô‡∏ö‡∏¢‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå\", engine=\"newmm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ']\n"
     ]
    }
   ],
   "source": [
    "# Enter you own setnence\n",
    "print(word_tokenize(\" \", engine=\"newmm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Question 2:__ Add your own custom dictionary for `newmm` tokenizer to tokenize the into the following tokens:\n",
    "\n",
    "```\n",
    "\n",
    "\"‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 22 ‡∏ï.‡∏Ñ. ‡πÄ‡∏≠‡πÄ‡∏≠‡∏ü‡∏û‡∏µ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ß‡πà‡∏≤ ‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥‡∏ô‡∏≤‡∏£‡∏∏‡∏Æ‡∏¥‡πÇ‡∏ï‡∏∞ ‡∏ó‡∏£‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏û‡∏¥‡∏ò‡∏µ‡∏ö‡∏£‡∏°‡∏£‡∏≤‡∏ä‡∏≤‡∏†‡∏¥‡πÄ‡∏©‡∏Å ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥‡πÅ‡∏´‡πà‡∏á‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡πÇ‡∏î‡∏¢‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡∏ó‡∏µ‡πà‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ß‡∏±‡∏á‡∏´‡∏•‡∏ß‡∏á‡πÉ‡∏ô‡∏Å‡∏£‡∏∏‡∏á‡πÇ‡∏ï‡πÄ‡∏Å‡∏µ‡∏¢‡∏ß\",\n",
    "\n",
    "```\n",
    "\n",
    "Result with the default dictionary:\n",
    "\n",
    "```\n",
    "['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', ' ', '22', ' ', '‡∏ï.‡∏Ñ.', ' ', '‡πÄ‡∏≠‡πÄ‡∏≠‡∏ü‡∏û‡∏µ', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô', '‡∏ß‡πà‡∏≤', ' ', '‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à', '‡∏û‡∏£‡∏∞', '‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥', '‡∏ô‡∏≤', '‡∏£‡∏∏', '‡∏Æ‡∏¥', '‡πÇ‡∏ï‡∏∞', ' ', '‡∏ó‡∏£‡∏á', '‡πÄ‡∏Ç‡πâ‡∏≤', '‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏û‡∏¥‡∏ò‡∏µ', '‡∏ö‡∏£‡∏°‡∏£‡∏≤‡∏ä‡∏≤‡∏†‡∏¥‡πÄ‡∏©‡∏Å', ' ', '‡πÄ‡∏õ‡πá‡∏ô', '‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à', '‡∏û‡∏£‡∏∞', '‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥', '‡πÅ‡∏´‡πà‡∏á', '‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô', '‡πÇ‡∏î‡∏¢', '‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå', '‡πÅ‡∏•‡πâ‡∏ß', '‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ', ' ', '‡∏ó‡∏µ‡πà', '‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ß‡∏±‡∏á', '‡∏´‡∏•‡∏ß‡∏á', '‡πÉ‡∏ô', '‡∏Å‡∏£‡∏∏‡∏á', '‡πÇ‡∏ï‡πÄ‡∏Å‡∏µ‡∏¢‡∏ß']\n",
    "```\n",
    "\n",
    "Expectation:\n",
    "```\n",
    "['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', ' ', '22', ' ', '‡∏ï.‡∏Ñ.', ' ', '‡πÄ‡∏≠‡πÄ‡∏≠‡∏ü‡∏û‡∏µ', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô', '‡∏ß‡πà‡∏≤', ' ', '‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥', '‡∏ô‡∏≤‡∏£‡∏∏‡∏Æ‡∏¥‡πÇ‡∏ï‡∏∞', ' ', '‡∏ó‡∏£‡∏á', '‡πÄ‡∏Ç‡πâ‡∏≤', '‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏û‡∏¥‡∏ò‡∏µ', '‡∏ö‡∏£‡∏°‡∏£‡∏≤‡∏ä‡∏≤‡∏†‡∏¥‡πÄ‡∏©‡∏Å', ' ', '‡πÄ‡∏õ‡πá‡∏ô', '‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥', '‡πÅ‡∏´‡πà‡∏á', '‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô', '‡πÇ‡∏î‡∏¢', '‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå', '‡πÅ‡∏•‡πâ‡∏ß', '‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ', ' ', '‡∏ó‡∏µ‡πà', '‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ß‡∏±‡∏á', '‡∏´‡∏•‡∏ß‡∏á', '‡πÉ‡∏ô', '‡∏Å‡∏£‡∏∏‡∏á‡πÇ‡∏ï‡πÄ‡∏Å‡∏µ‡∏¢‡∏ß']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize.trie import Trie\n",
    "from pythainlp.corpus import thai_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_from_news = \"\"\"‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 22 ‡∏ï.‡∏Ñ. ‡πÄ‡∏≠‡πÄ‡∏≠‡∏ü‡∏û‡∏µ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ß‡πà‡∏≤ ‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥‡∏ô‡∏≤‡∏£‡∏∏‡∏Æ‡∏¥‡πÇ‡∏ï‡∏∞ ‡∏ó‡∏£‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏û‡∏¥‡∏ò‡∏µ‡∏ö‡∏£‡∏°‡∏£‡∏≤‡∏ä‡∏≤‡∏†‡∏¥‡πÄ‡∏©‡∏Å ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥‡πÅ‡∏´‡πà‡∏á‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡πÇ‡∏î‡∏¢‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡∏ó‡∏µ‡πà‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ß‡∏±‡∏á‡∏´‡∏•‡∏ß‡∏á‡πÉ‡∏ô‡∏Å‡∏£‡∏∏‡∏á‡πÇ‡∏ï‡πÄ‡∏Å‡∏µ‡∏¢‡∏ß\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add vocab in this list\n",
    "custom_vocab = [\n",
    "    \n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenize_japan_news(custom_vocab):\n",
    "    expect = ['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', ' ', '22', ' ', '‡∏ï.‡∏Ñ.', ' ', '‡πÄ‡∏≠‡πÄ‡∏≠‡∏ü‡∏û‡∏µ', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô', '‡∏ß‡πà‡∏≤', ' ',\n",
    "              '‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥', '‡∏ô‡∏≤‡∏£‡∏∏‡∏Æ‡∏¥‡πÇ‡∏ï‡∏∞', ' ', '‡∏ó‡∏£‡∏á', '‡πÄ‡∏Ç‡πâ‡∏≤', '‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏û‡∏¥‡∏ò‡∏µ', '‡∏ö‡∏£‡∏°‡∏£‡∏≤‡∏ä‡∏≤‡∏†‡∏¥‡πÄ‡∏©‡∏Å',\n",
    "              ' ', '‡πÄ‡∏õ‡πá‡∏ô', '‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥', '‡πÅ‡∏´‡πà‡∏á', '‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô', '‡πÇ‡∏î‡∏¢', '‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå', '‡πÅ‡∏•‡πâ‡∏ß', '‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ',\n",
    "              ' ', '‡∏ó‡∏µ‡πà', '‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ß‡∏±‡∏á', '‡∏´‡∏•‡∏ß‡∏á', '‡πÉ‡∏ô', '‡∏Å‡∏£‡∏∏‡∏á‡πÇ‡∏ï‡πÄ‡∏Å‡∏µ‡∏¢‡∏ß']\n",
    "    \n",
    "    custom_dict_trie = Trie( list(thai_words()) + custom_vocab)\n",
    "\n",
    "    actual = word_tokenize(text_from_news, custom_dict=custom_dict_trie, engine=\"newmm\")\n",
    "    \n",
    "    \n",
    "   \n",
    "    if actual == expect:\n",
    "        print(\"‚úÖ Test succeed. üòÅ\")\n",
    "    else:\n",
    "        print(\"‚ùå Test failed. üò≠\")\n",
    "        print(\"\\nYour result    :\\n\\n\", \"|\".join(actual))\n",
    "        print(\"\\nExtected result:\\n\\n\", \"|\".join(expect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Test failed. üò≠\n",
      "\n",
      "Your result    :\n",
      "\n",
      " ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà| |22| |‡∏ï.‡∏Ñ.| |‡πÄ‡∏≠‡πÄ‡∏≠‡∏ü‡∏û‡∏µ|‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô|‡∏ß‡πà‡∏≤| |‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à|‡∏û‡∏£‡∏∞|‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥|‡∏ô‡∏≤|‡∏£‡∏∏|‡∏Æ‡∏¥|‡πÇ‡∏ï‡∏∞| |‡∏ó‡∏£‡∏á|‡πÄ‡∏Ç‡πâ‡∏≤|‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏û‡∏¥‡∏ò‡∏µ|‡∏ö‡∏£‡∏°‡∏£‡∏≤‡∏ä‡∏≤‡∏†‡∏¥‡πÄ‡∏©‡∏Å| |‡πÄ‡∏õ‡πá‡∏ô|‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à|‡∏û‡∏£‡∏∞|‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥|‡πÅ‡∏´‡πà‡∏á|‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô|‡πÇ‡∏î‡∏¢|‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå|‡πÅ‡∏•‡πâ‡∏ß|‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ| |‡∏ó‡∏µ‡πà|‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ß‡∏±‡∏á|‡∏´‡∏•‡∏ß‡∏á|‡πÉ‡∏ô|‡∏Å‡∏£‡∏∏‡∏á|‡πÇ‡∏ï‡πÄ‡∏Å‡∏µ‡∏¢‡∏ß\n",
      "\n",
      "Extected result:\n",
      "\n",
      " ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà| |22| |‡∏ï.‡∏Ñ.| |‡πÄ‡∏≠‡πÄ‡∏≠‡∏ü‡∏û‡∏µ|‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô|‡∏ß‡πà‡∏≤| |‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥|‡∏ô‡∏≤‡∏£‡∏∏‡∏Æ‡∏¥‡πÇ‡∏ï‡∏∞| |‡∏ó‡∏£‡∏á|‡πÄ‡∏Ç‡πâ‡∏≤|‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏û‡∏¥‡∏ò‡∏µ|‡∏ö‡∏£‡∏°‡∏£‡∏≤‡∏ä‡∏≤‡∏†‡∏¥‡πÄ‡∏©‡∏Å| |‡πÄ‡∏õ‡πá‡∏ô|‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥|‡πÅ‡∏´‡πà‡∏á|‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô|‡πÇ‡∏î‡∏¢|‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå|‡πÅ‡∏•‡πâ‡∏ß|‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ| |‡∏ó‡∏µ‡πà|‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ß‡∏±‡∏á|‡∏´‡∏•‡∏ß‡∏á|‡πÉ‡∏ô|‡∏Å‡∏£‡∏∏‡∏á‡πÇ‡∏ï‡πÄ‡∏Å‡∏µ‡∏¢‡∏ß\n"
     ]
    }
   ],
   "source": [
    "test_tokenize_japan_news(custom_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test succeed. üòÅ\n"
     ]
    }
   ],
   "source": [
    "# Add vocab\n",
    "custom_vocab = [\n",
    "    \"‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥\",\n",
    "    \"‡∏Å‡∏£‡∏∏‡∏á‡πÇ‡∏ï‡πÄ‡∏Å‡∏µ‡∏¢‡∏ß\",\n",
    "    \"‡∏ô‡∏≤‡∏£‡∏∏‡∏Æ‡∏¥‡πÇ‡∏ï‡∏∞\"\n",
    "]\n",
    "\n",
    "test_tokenize_japan_news(custom_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Learning-based tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer is a Machine Learning model and train on supervised daataset (labeled dataset).\n",
    "\n",
    "\n",
    "For example, one tokenizer of PyThaiNLP (`attacut`) uses Convolutional-neural Network to read the whole text and then determind word boundaries.\n",
    "\n",
    "![attacut](images/attacut.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attacut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q attacut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°', '‡πÅ‡∏•‡∏∞', '‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Å‡∏≤‡∏ï‡∏≤‡∏£‡πå', ' ', '‡∏à‡∏±‡∏î', '‡∏á‡∏≤‡∏ô', ' ', 'Qatar', ' ', 'Information', ' ', 'Technology', ' ', 'Exhibition', ' ', 'and', ' ', 'Conference', ' ', '(', 'QITCOM', ' ', '2019', ')']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Å‡∏≤‡∏ï‡∏≤‡∏£‡πå ‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô Qatar Information Technology Exhibition and Conference (QITCOM 2019)\"\n",
    "\n",
    "tokens = word_tokenize(test_sentence, engine=\"attacut\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡∏â‡∏±‡∏ô', '‡∏≠‡∏¢‡∏∏‡πã', '‡∏ó‡∏µ‡πà', ' ', '‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï‡∏û‡∏±‡∏í‡∏ô‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"‡∏â‡∏±‡∏ô‡∏≠‡∏¢‡∏∏‡πã‡∏ó‡∏µ‡πà ‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï‡∏û‡∏±‡∏í‡∏ô‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå\"\n",
    "\n",
    "tokens = word_tokenize(test_sentence, engine=\"attacut\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Try out:__ Try adding your own sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ']\n"
     ]
    }
   ],
   "source": [
    "# Enter you own setnence\n",
    "print(word_tokenize(\" \", engine=\"attacut\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 2. Part of speech and Named Entity Recognition Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tag.named_entity import ThaiNameTagger\n",
    "\n",
    "tagger = ThaiNameTagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entitiy Regcognition (NER) Tags:\n",
    "\n",
    "|       Tags       |      Examples                       |\n",
    "|------------------|-------------------------------------|\n",
    "        DATE       |   1 ‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏° 2012                      |\n",
    "        EMAIL      |   hr@mycompany.com                  |    \n",
    "        LAW        |  ‡∏û‡∏£‡∏ö.‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ                   |\n",
    "        LEN        |       80 ‡∏Å‡∏¥‡πÇ‡∏•‡πÄ‡∏°‡∏ï‡∏£                    |     \n",
    "      LOCATION     |  ‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û, ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏à‡∏µ‡∏ô, ‡πÄ‡∏≠‡πÄ‡∏ß‡∏≠‡πÄ‡∏£‡∏™‡∏ï‡πå        | \n",
    "        MONEY      |   2,190 ‡∏•‡πâ‡∏≤‡∏ô‡∏ö‡∏≤‡∏ó                      |\n",
    "    ORGANIZATION   |  ‡∏Ñ‡∏ì‡∏∞‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏à‡∏∏‡∏¨‡∏≤‡∏•‡∏á‡∏Å‡∏£‡∏ì‡πå‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢     |\n",
    "       PERCENT     |   95.34%, 10‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡∏ô‡∏ï‡πå                |\n",
    "       PERSON      |   ‡∏≠‡∏£‡∏£‡∏ñ‡∏û‡∏• ‡∏ò‡∏≥‡∏£‡∏á‡∏£‡∏±‡∏ï‡∏ô‡∏§‡∏ó‡∏ò‡∏¥‡πå                 |\n",
    "        PHONE      |   +6611-123-1123                    |\n",
    "         TIME      |      14:20 ‡∏ô, ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á‡∏ï‡∏£‡∏á           |\n",
    "          URL      |     mycompany.com                   |\n",
    "         ZIP       |     ‡∏£‡∏´‡∏±‡∏™‡πÑ‡∏õ‡∏£‡∏ì‡∏µ‡∏¢‡πå 21210                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏° ‡πÑ‡∏î‡πâ‡πÑ‡∏õ‡∏á‡∏≤‡∏ô‡πÄ‡∏õ‡∏¥‡∏î‡∏ö‡πâ‡∏≤‡∏ô ‡∏ó‡∏µ‡πà‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ', 'B-DATE'),\n",
       " (' ', 'O'),\n",
       " ('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'O'),\n",
       " (' ', 'O'),\n",
       " ('1', 'B-DATE'),\n",
       " (' ', 'I-DATE'),\n",
       " ('‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°', 'I-DATE'),\n",
       " (' ', 'O'),\n",
       " ('‡πÑ‡∏î‡πâ', 'O'),\n",
       " ('‡πÑ‡∏õ', 'O'),\n",
       " ('‡∏á‡∏≤‡∏ô', 'O'),\n",
       " ('‡πÄ‡∏õ‡∏¥‡∏î', 'O'),\n",
       " ('‡∏ö‡πâ‡∏≤‡∏ô', 'O'),\n",
       " (' ', 'O'),\n",
       " ('‡∏ó‡∏µ‡πà', 'O'),\n",
       " ('‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå', 'B-LOCATION')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.get_ner(sentence, pos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ', 'NOUN', 'B-DATE'),\n",
       " (' ', 'PUNCT', 'O'),\n",
       " ('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'NOUN', 'O'),\n",
       " (' ', 'PUNCT', 'O'),\n",
       " ('1', 'NUM', 'B-DATE'),\n",
       " (' ', 'PUNCT', 'I-DATE'),\n",
       " ('‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°', 'NOUN', 'I-DATE'),\n",
       " (' ', 'PUNCT', 'O'),\n",
       " ('‡πÑ‡∏î‡πâ', 'AUX', 'O'),\n",
       " ('‡πÑ‡∏õ', 'VERB', 'O'),\n",
       " ('‡∏á‡∏≤‡∏ô', 'NOUN', 'O'),\n",
       " ('‡πÄ‡∏õ‡∏¥‡∏î', 'VERB', 'O'),\n",
       " ('‡∏ö‡πâ‡∏≤‡∏ô', 'NOUN', 'O'),\n",
       " (' ', 'PUNCT', 'O'),\n",
       " ('‡∏ó‡∏µ‡πà', 'SCONJ', 'O'),\n",
       " ('‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå', 'NOUN', 'B-LOCATION')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.get_ner(sentence, pos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Question 3:__ From the following setentences how many types of named-entity appear in the sentence\n",
    "\n",
    "\n",
    "```text\n",
    "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡πì‡πë ‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏° ‡πí‡πï‡πñ‡πí ‡πÄ‡∏ß‡∏•‡∏≤ 13:00 ‡∏ô. ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‡πÄ‡∏°‡∏∑‡πà‡∏≠', 'O'),\n",
       " ('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'O'),\n",
       " (' ', 'O'),\n",
       " ('‡πì‡πë', 'B-DATE'),\n",
       " (' ', 'I-DATE'),\n",
       " ('‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°', 'I-DATE'),\n",
       " (' ', 'I-DATE'),\n",
       " ('‡πí‡πï‡πñ‡πí', 'I-DATE'),\n",
       " (' ', 'O'),\n",
       " ('‡πÄ‡∏ß‡∏•‡∏≤', 'O'),\n",
       " (' ', 'O'),\n",
       " ('13', 'B-TIME'),\n",
       " (':', 'I-TIME'),\n",
       " ('00', 'I-TIME'),\n",
       " (' ', 'I-TIME'),\n",
       " ('‡∏ô.', 'I-TIME'),\n",
       " (' ', 'O'),\n",
       " ('‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤', 'O'),\n",
       " ('‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®', 'O'),\n",
       " ('‡πÑ‡∏ó‡∏¢', 'B-LOCATION')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.get_ner(\"‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡πì‡πë ‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏° ‡πí‡πï‡πñ‡πí ‡πÄ‡∏ß‡∏•‡∏≤ 13:00 ‡∏ô. ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\", pos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Question 4:__ From the following setentences how many types of named-entity appear in the sentence\n",
    "\n",
    "Reference: https://www.khaosod.co.th/around-the-world-news/news_2993136\n",
    "\n",
    "```text\n",
    "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 22 ‡∏ï.‡∏Ñ. ‡πÄ‡∏≠‡πÄ‡∏≠‡∏ü‡∏û‡∏µ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ß‡πà‡∏≤ ‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥‡∏ô‡∏≤‡∏£‡∏∏‡∏Æ‡∏¥‡πÇ‡∏ï‡∏∞ ‡∏ó‡∏£‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏û‡∏¥‡∏ò‡∏µ‡∏ö‡∏£‡∏°‡∏£‡∏≤‡∏ä‡∏≤‡∏†‡∏¥‡πÄ‡∏©‡∏Å\n",
    "‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥‡πÅ‡∏´‡πà‡∏á‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡πÇ‡∏î‡∏¢‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡∏ó‡∏µ‡πà‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ß‡∏±‡∏á‡∏´‡∏•‡∏ß‡∏á‡πÉ‡∏ô‡∏Å‡∏£‡∏∏‡∏á‡πÇ‡∏ï‡πÄ‡∏Å‡∏µ‡∏¢‡∏ß\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'O'),\n",
       " (' ', 'O'),\n",
       " ('22', 'B-DATE'),\n",
       " (' ', 'I-DATE'),\n",
       " ('‡∏ï.‡∏Ñ.', 'I-DATE'),\n",
       " (' ', 'O'),\n",
       " ('‡πÄ‡∏≠‡πÄ‡∏≠‡∏ü‡∏û‡∏µ', 'B-ORGANIZATION'),\n",
       " ('‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô', 'O'),\n",
       " ('‡∏ß‡πà‡∏≤', 'O'),\n",
       " (' ', 'O'),\n",
       " ('‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à', 'B-PERSON'),\n",
       " ('‡∏û‡∏£‡∏∞', 'I-PERSON'),\n",
       " ('‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥', 'I-PERSON'),\n",
       " ('‡∏ô‡∏≤', 'I-PERSON'),\n",
       " ('‡∏£‡∏∏', 'I-PERSON'),\n",
       " ('‡∏Æ‡∏¥', 'I-PERSON'),\n",
       " ('‡πÇ‡∏ï‡∏∞', 'I-PERSON'),\n",
       " (' ', 'O'),\n",
       " ('‡∏ó‡∏£‡∏á', 'O'),\n",
       " ('‡πÄ‡∏Ç‡πâ‡∏≤', 'O'),\n",
       " ('‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏û‡∏¥‡∏ò‡∏µ', 'O'),\n",
       " ('‡∏ö‡∏£‡∏°‡∏£‡∏≤‡∏ä‡∏≤‡∏†‡∏¥‡πÄ‡∏©‡∏Å', 'O'),\n",
       " (' ', 'O'),\n",
       " ('‡πÄ‡∏õ‡πá‡∏ô', 'O'),\n",
       " ('‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à', 'O'),\n",
       " ('‡∏û‡∏£‡∏∞', 'O'),\n",
       " ('‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥', 'O'),\n",
       " ('‡πÅ‡∏´‡πà‡∏á', 'O'),\n",
       " ('‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô', 'B-LOCATION'),\n",
       " ('‡πÇ‡∏î‡∏¢', 'O'),\n",
       " ('‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå', 'O'),\n",
       " ('‡πÅ‡∏•‡πâ‡∏ß', 'O'),\n",
       " ('‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ', 'B-DATE'),\n",
       " (' ', 'O'),\n",
       " ('‡∏ó‡∏µ‡πà', 'O'),\n",
       " ('‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ß‡∏±‡∏á', 'B-LOCATION'),\n",
       " ('‡∏´‡∏•‡∏ß‡∏á', 'I-LOCATION'),\n",
       " ('‡πÉ‡∏ô', 'O'),\n",
       " ('‡∏Å‡∏£‡∏∏‡∏á', 'B-LOCATION'),\n",
       " ('‡πÇ‡∏ï‡πÄ‡∏Å‡∏µ‡∏¢‡∏ß', 'I-LOCATION')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out\n",
    "tagger.get_ner(\"‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 22 ‡∏ï.‡∏Ñ. ‡πÄ‡∏≠‡πÄ‡∏≠‡∏ü‡∏û‡∏µ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ß‡πà‡∏≤ ‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥‡∏ô‡∏≤‡∏£‡∏∏‡∏Æ‡∏¥‡πÇ‡∏ï‡∏∞ ‡∏ó‡∏£‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏û‡∏¥‡∏ò‡∏µ‡∏ö‡∏£‡∏°‡∏£‡∏≤‡∏ä‡∏≤‡∏†‡∏¥‡πÄ‡∏©‡∏Å ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏£‡∏û‡∏£‡∏£‡∏î‡∏¥‡πÅ‡∏´‡πà‡∏á‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡πÇ‡∏î‡∏¢‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡∏ó‡∏µ‡πà‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ß‡∏±‡∏á‡∏´‡∏•‡∏ß‡∏á‡πÉ‡∏ô‡∏Å‡∏£‡∏∏‡∏á‡πÇ‡∏ï‡πÄ‡∏Å‡∏µ‡∏¢‡∏ß\", pos=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Question 5:__ From the following setentences how many types of named-entity appear in the sentence\n",
    "\n",
    "Reference: [link](http://www.arts.chula.ac.th/ling/blog/tag/%E0%B8%AD%E0%B8%A3%E0%B8%A3%E0%B8%96%E0%B8%9E%E0%B8%A5-%E0%B8%98%E0%B8%B3%E0%B8%A3%E0%B8%87%E0%B8%A3%E0%B8%B1%E0%B8%95%E0%B8%99%E0%B8%A4%E0%B8%97%E0%B8%98%E0%B8%B4%E0%B9%8C/)\n",
    "\n",
    "```text\n",
    "‡∏Ñ‡∏ì‡∏∞‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏à‡∏∏‡∏¨‡∏≤‡∏•‡∏á‡∏Å‡∏£‡∏ì‡πå‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢ ‡∏Ç‡∏≠‡πÄ‡∏ä‡∏¥‡∏ç‡∏ä‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏™‡∏ô‡πÉ‡∏à‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°‡∏ü‡∏±‡∏á‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏û‡∏¥‡πÄ‡∏®‡∏©\n",
    "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á ‚Äú‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏õ‡∏£‡∏¥‡∏à‡πÄ‡∏â‡∏ó‡πÅ‡∏ö‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Ñ‡∏≥‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‚Äù\n",
    "‡πÇ‡∏î‡∏¢ ‡∏î‡∏£.‡∏≠‡∏£‡∏£‡∏ñ‡∏û‡∏• ‡∏ò‡∏≥‡∏£‡∏á‡∏£‡∏±‡∏ï‡∏ô‡∏§‡∏ó‡∏ò‡∏¥‡πå\n",
    "\n",
    "‡∏ß‡∏±‡∏ô‡∏®‡∏∏‡∏Å‡∏£‡πå‡∏ó‡∏µ‡πà 17 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô 2560 ‡πÄ‡∏ß‡∏•‡∏≤ 13.30-14.30 ‡∏ô.\n",
    "‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏õ ‡∏ì ‡∏´‡πâ‡∏≠‡∏á 401/5 ‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏°‡∏´‡∏≤‡∏à‡∏±‡∏Å‡∏£‡∏µ‡∏™‡∏¥‡∏£‡∏¥‡∏ô‡∏ò‡∏£ ‡∏Ñ‡∏ì‡∏∞‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏à‡∏∏‡∏¨‡∏≤‡∏•‡∏á‡∏Å‡∏£‡∏ì‡πå‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢\n",
    "\n",
    "‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà 0-2218-4692\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‡∏Ñ‡∏ì‡∏∞‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå', 'B-ORGANIZATION'),\n",
       " (' ', 'O'),\n",
       " ('‡∏à‡∏∏‡∏¨‡∏≤‡∏•‡∏á‡∏Å‡∏£‡∏ì‡πå', 'B-ORGANIZATION'),\n",
       " ('‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', 'I-ORGANIZATION'),\n",
       " ('\\n', 'O'),\n",
       " ('‡∏Ç‡∏≠', 'O'),\n",
       " ('‡πÄ‡∏ä‡∏¥‡∏ç‡∏ä‡∏ß‡∏ô', 'O'),\n",
       " ('‡∏ú‡∏π‡πâ', 'O'),\n",
       " ('‡∏™‡∏ô‡πÉ‡∏à', 'O'),\n",
       " ('‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°', 'O'),\n",
       " ('‡∏ü‡∏±‡∏á', 'O'),\n",
       " ('‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢', 'O'),\n",
       " ('‡∏û‡∏¥‡πÄ‡∏®‡∏©', 'O'),\n",
       " (' ', 'O'),\n",
       " ('\\n', 'O'),\n",
       " ('‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á', 'O'),\n",
       " (' ', 'O'),\n",
       " ('‚Äú', 'O'),\n",
       " ('‡∏Å‡∏≤‡∏£', 'O'),\n",
       " ('‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå', 'O'),\n",
       " ('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå', 'O'),\n",
       " ('‡∏†‡∏≤‡∏¢‡πÉ‡∏ô', 'O'),\n",
       " ('‡∏õ‡∏£‡∏¥', 'O'),\n",
       " ('‡∏à', 'O'),\n",
       " ('‡πÄ‡∏â‡∏ó', 'O'),\n",
       " ('‡πÅ‡∏ö‡∏ö', 'O'),\n",
       " ('‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥', 'O'),\n",
       " ('‡∏î‡πâ‡∏ß‡∏¢', 'O'),\n",
       " ('‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å', 'O'),\n",
       " ('‡∏Ñ‡∏≥‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°', 'O'),\n",
       " ('‚Äù', 'O'),\n",
       " ('\\n', 'O'),\n",
       " ('‡πÇ‡∏î‡∏¢', 'O'),\n",
       " (' ', 'O'),\n",
       " ('‡∏î‡∏£.', 'B-PERSON'),\n",
       " ('‡∏≠‡∏£‡∏£‡∏ñ', 'I-PERSON'),\n",
       " ('‡∏û‡∏•', 'I-PERSON'),\n",
       " (' ', 'I-PERSON'),\n",
       " ('‡∏ò‡∏≥‡∏£‡∏á', 'I-PERSON'),\n",
       " ('‡∏£‡∏±‡∏ï‡∏ô', 'I-PERSON'),\n",
       " ('‡∏§‡∏ó‡∏ò‡∏¥‡πå', 'I-PERSON'),\n",
       " ('\\n', 'I-PERSON'),\n",
       " ('‡∏ß‡∏±‡∏ô', 'I-PERSON'),\n",
       " ('‡∏®‡∏∏‡∏Å‡∏£‡πå', 'I-PERSON'),\n",
       " ('‡∏ó‡∏µ‡πà', 'O'),\n",
       " (' ', 'O'),\n",
       " ('17', 'B-DATE'),\n",
       " (' ', 'I-DATE'),\n",
       " ('‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô', 'I-DATE'),\n",
       " (' ', 'I-DATE'),\n",
       " ('2560', 'I-DATE'),\n",
       " (' ', 'O'),\n",
       " ('‡πÄ‡∏ß‡∏•‡∏≤', 'O'),\n",
       " (' ', 'O'),\n",
       " ('13.30', 'B-TIME'),\n",
       " ('-', 'I-TIME'),\n",
       " ('14.30', 'I-TIME'),\n",
       " (' ', 'I-TIME'),\n",
       " ('‡∏ô.', 'I-TIME'),\n",
       " (' ', 'O'),\n",
       " ('‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏õ', 'O'),\n",
       " (' ', 'O'),\n",
       " ('‡∏ì', 'O'),\n",
       " (' ', 'O'),\n",
       " ('‡∏´‡πâ‡∏≠‡∏á', 'O'),\n",
       " (' ', 'O'),\n",
       " ('401', 'O'),\n",
       " ('/', 'O'),\n",
       " ('5', 'O'),\n",
       " (' ', 'O'),\n",
       " ('‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£', 'B-LOCATION'),\n",
       " ('‡∏°‡∏´‡∏≤', 'I-LOCATION'),\n",
       " ('‡∏à‡∏±‡∏Å‡∏£‡∏µ', 'I-LOCATION'),\n",
       " ('‡∏™‡∏¥‡∏£‡∏¥‡∏ô‡∏ò‡∏£', 'I-LOCATION'),\n",
       " (' ', 'O'),\n",
       " ('‡∏Ñ‡∏ì‡∏∞‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå', 'O'),\n",
       " (' ', 'O'),\n",
       " ('‡∏à‡∏∏‡∏¨‡∏≤‡∏•‡∏á‡∏Å‡∏£‡∏ì‡πå', 'B-ORGANIZATION'),\n",
       " ('‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', 'I-ORGANIZATION'),\n",
       " ('\\n', 'O'),\n",
       " ('‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°', 'O'),\n",
       " ('‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î', 'O'),\n",
       " ('‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°', 'O'),\n",
       " ('‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà', 'O'),\n",
       " (' ', 'O'),\n",
       " ('0', 'B-PHONE'),\n",
       " ('-', 'I-PHONE'),\n",
       " ('2218', 'I-PHONE'),\n",
       " ('-', 'I-PHONE'),\n",
       " ('4692', 'I-PHONE'),\n",
       " ('\\n', 'I-PHONE')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"‡∏Ñ‡∏ì‡∏∞‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏à‡∏∏‡∏¨‡∏≤‡∏•‡∏á‡∏Å‡∏£‡∏ì‡πå‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢\n",
    "‡∏Ç‡∏≠‡πÄ‡∏ä‡∏¥‡∏ç‡∏ä‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏™‡∏ô‡πÉ‡∏à‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°‡∏ü‡∏±‡∏á‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏û‡∏¥‡πÄ‡∏®‡∏© \n",
    "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á ‚Äú‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏õ‡∏£‡∏¥‡∏à‡πÄ‡∏â‡∏ó‡πÅ‡∏ö‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Ñ‡∏≥‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‚Äù\n",
    "‡πÇ‡∏î‡∏¢ ‡∏î‡∏£.‡∏≠‡∏£‡∏£‡∏ñ‡∏û‡∏• ‡∏ò‡∏≥‡∏£‡∏á‡∏£‡∏±‡∏ï‡∏ô‡∏§‡∏ó‡∏ò‡∏¥‡πå\n",
    "‡∏ß‡∏±‡∏ô‡∏®‡∏∏‡∏Å‡∏£‡πå‡∏ó‡∏µ‡πà 17 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô 2560 ‡πÄ‡∏ß‡∏•‡∏≤ 13.30-14.30 ‡∏ô. ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏õ ‡∏ì ‡∏´‡πâ‡∏≠‡∏á 401/5 ‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏°‡∏´‡∏≤‡∏à‡∏±‡∏Å‡∏£‡∏µ‡∏™‡∏¥‡∏£‡∏¥‡∏ô‡∏ò‡∏£ ‡∏Ñ‡∏ì‡∏∞‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏à‡∏∏‡∏¨‡∏≤‡∏•‡∏á‡∏Å‡∏£‡∏ì‡πå‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢\n",
    "‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà 0-2218-4692\n",
    "\"\"\"\n",
    "\n",
    "tagger.get_ner(text, pos=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of Speech (POS) Tags:\n",
    "\n",
    "\n",
    "Reference: [PUD Tags](https://universaldependencies.org/u/pos/all.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "|  Abbreviation |      Part-of-Speech tag    |            Examples             |       \n",
    "|---------------|----------------------------|---------------------------------|\n",
    "| ADJ           |  Adjective                 |    ‡πÉ‡∏´‡∏°‡πà, ‡∏û‡∏¥‡πÄ‡∏®‡∏© , ‡∏Å‡πà‡∏≠‡∏ô, ‡∏°‡∏≤‡∏Å, ‡∏™‡∏π‡∏á     |   \n",
    "| ADP           |  Adposition                |   ‡πÅ‡∏°‡πâ, ‡∏ß‡πà‡∏≤, ‡πÄ‡∏°‡∏∑‡πà‡∏≠, ‡∏Ç‡∏≠‡∏á, ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö       |   \n",
    "| ADV           |  Adverb                    |   ‡∏Å‡πà‡∏≠‡∏ô, ‡∏Å‡πá, ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢, ‡πÄ‡∏•‡∏¢, ‡∏™‡∏∏‡∏î       |   \n",
    "| AUX           |  Auxiliary                 |   ‡πÄ‡∏õ‡πá‡∏ô, ‡πÉ‡∏ä‡πà, ‡∏Ñ‡∏∑‡∏≠, ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢             |   \n",
    "| CCONJ         |  Coordinating conjunction  |   ‡πÅ‡∏ï‡πà, ‡πÅ‡∏•‡∏∞, ‡∏´‡∏£‡∏∑‡∏≠                  |        \n",
    "| DET           |  Determiner                |   ‡∏ô‡∏µ‡πâ, ‡∏ô‡∏±‡πâ‡∏ô, ‡∏ó‡∏±‡πâ‡∏á, ‡πÄ‡∏û‡∏µ‡∏¢‡∏á, (‡∏´‡∏ô‡∏∂‡πà‡∏á)‡∏Ñ‡∏ô      |   \n",
    "| INTJ          |  Interjection              |   ‡∏≠‡∏∏‡πâ‡∏¢, ‡πÇ‡∏≠‡πâ‡∏¢                       |   \n",
    "| NOUN          |  Noun                      |   ‡∏Å‡∏≥‡∏°‡∏∑‡∏≠, ‡∏û‡∏ß‡∏Å, ‡∏™‡∏ô‡∏≤‡∏°, ‡∏Å‡∏µ‡∏¨‡∏≤, ‡∏ö‡∏±‡∏ç‡∏ä‡∏µ     |   \n",
    "| NUM           |  Numeral                   |   5,000, 103.7, 2004, ‡∏´‡∏ô‡∏∂‡πà‡∏á, ‡∏£‡πâ‡∏≠‡∏¢  |   \n",
    "| PART          |  Particle                  |   ‡∏°‡∏≤ ‡∏Ç‡∏∂‡πâ‡∏ô ‡πÑ‡∏°‡πà ‡πÑ‡∏î‡πâ ‡πÄ‡∏Ç‡πâ‡∏≤               |      \n",
    "| PRON          |  Pronoun                   |   ‡πÄ‡∏£‡∏≤, ‡πÄ‡∏Ç‡∏≤, ‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á, ‡πÉ‡∏Ñ‡∏£, ‡πÄ‡∏ò‡∏≠     |   \n",
    "| PROPN         |  Proper noun               |   ‡πÇ‡∏≠‡∏ö‡∏≤‡∏°‡∏≤, ‡πÅ‡∏Ñ‡∏õ‡∏¥‡∏ï‡∏≠‡∏•‡∏Æ‡∏¥‡∏•, ‡∏à‡∏µ‡πÇ‡∏≠‡∏û‡∏µ, ‡πÑ‡∏°‡πÄ‡∏Ñ‡∏¥‡∏• |   \n",
    "| PUNCT         |  Punctuation               |   (, ), \", ', :                 |    \n",
    "| SCONJ         |  Subordinating conjunction |    ‡∏´‡∏≤‡∏Å, ‡πÄ‡∏û‡πà‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤, ‡∏ñ‡πâ‡∏≤             |   \n",
    "| VERB          |  Verb                      |   ‡πÄ‡∏õ‡∏¥‡∏î, ‡πÉ‡∏´‡πâ, ‡πÉ‡∏ä‡πâ, ‡πÄ‡∏ú‡∏ä‡∏¥‡∏ç, ‡∏≠‡πà‡∏≤‡∏ô        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‡∏â‡∏±‡∏ô', 'PRON'),\n",
       " ('‡πÑ‡∏õ', 'VERB'),\n",
       " ('‡πÄ‡∏î‡∏¥‡∏ô', 'VERB'),\n",
       " ('‡πÉ‡∏ô', 'ADP'),\n",
       " ('‡∏™‡∏ß‡∏ô‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞', 'NOUN')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentence = \"‡∏â‡∏±‡∏ô‡πÑ‡∏õ‡πÄ‡∏î‡∏¥‡∏ô‡πÉ‡∏ô‡∏™‡∏ß‡∏ô‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞\"\n",
    "tokens = word_tokenize(sentence, keep_whitespace=False)\n",
    "pos_tag(tokens, corpus=\"pud\", engine=\"perceptron\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "Explaination:\n",
    "\n",
    "PRON = Pronoun \n",
    "\n",
    "VERB = Verb\n",
    "\n",
    "ADP = Adposition\n",
    "\n",
    "NOUN = Noun\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‡∏â‡∏±‡∏ô', 'PPRS'),\n",
       " ('‡πÑ‡∏õ', 'VACT'),\n",
       " ('‡πÄ‡∏î‡∏¥‡∏ô', 'VACT'),\n",
       " ('‡πÉ‡∏ô', 'RPRE'),\n",
       " ('‡∏™‡∏ß‡∏ô‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞', 'NCMN')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentence = \"‡∏â‡∏±‡∏ô‡πÑ‡∏õ‡πÄ‡∏î‡∏¥‡∏ô‡πÉ‡∏ô‡∏™‡∏ß‡∏ô‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞\"\n",
    "tokens = word_tokenize(sentence, keep_whitespace=False)\n",
    "pos_tag(tokens, corpus=\"orchid\", engine=\"perceptron\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Explaination:\n",
    "\n",
    "PPRS = Personal pronoun \n",
    "\n",
    "VACT = Active verb\n",
    "\n",
    "RPRE = Preposition\n",
    "\n",
    "NCMN = Common noun\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Question 6:__ From the following setentences what are the POS tags (based on UD)\n",
    "\n",
    "\n",
    "```text\n",
    "‡∏´‡∏°‡∏≤ ‡πÅ‡∏•‡∏∞ ‡πÅ‡∏°‡∏ß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏¥‡∏ô ‡∏≠‡∏≤‡∏´‡∏≤‡∏£\n",
    "```\n",
    "\n",
    "\n",
    "Hint: Here is the list of POS tags of this sentence.\n",
    "\n",
    "- NOUN = Noun\n",
    "- CCONJ = Coordinating Conjunction\n",
    "- VERB = Active Verb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‡∏´‡∏°‡∏≤', 'NOUN'),\n",
       " ('‡πÅ‡∏•‡∏∞', 'CCONJ'),\n",
       " ('‡πÅ‡∏°‡∏ß', 'NOUN'),\n",
       " ('‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏¥‡∏ô', 'VERB'),\n",
       " ('‡∏≠‡∏≤‡∏´‡∏≤‡∏£', 'NOUN')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this block to see the result\n",
    "\n",
    "sentence = \"‡∏´‡∏°‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏°‡∏ß‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏¥‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£\"\n",
    "tokens = word_tokenize(sentence, keep_whitespace=False)\n",
    "pos_tag(tokens, corpus=\"ud\", engine=\"perceptron\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Question 7:__ From the following setentences what are the POS tags (based on Orchid)\n",
    "\n",
    "\n",
    "```text\n",
    "‡∏´‡∏°‡∏≤ ‡πÅ‡∏•‡∏∞ ‡πÅ‡∏°‡∏ß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏¥‡∏ô ‡∏≠‡∏≤‡∏´‡∏≤‡∏£\n",
    "```\n",
    "\n",
    "Hint: Here is the list of POS tags of this sentence.\n",
    "\n",
    "- NCMN = Common Noun\n",
    "- JCRG = Coordinating Conjunction\n",
    "- VACT = Active Verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‡∏´‡∏°‡∏≤', 'NCMN'),\n",
       " ('‡πÅ‡∏•‡∏∞', 'JCRG'),\n",
       " ('‡πÅ‡∏°‡∏ß', 'NCMN'),\n",
       " ('‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏¥‡∏ô', 'VACT'),\n",
       " ('‡∏≠‡∏≤‡∏´‡∏≤‡∏£', 'NCMN')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this block to see the result\n",
    "\n",
    "sentence = \"‡∏´‡∏°‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏°‡∏ß‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏¥‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£\"\n",
    "tokens = word_tokenize(sentence, keep_whitespace=False)\n",
    "pos_tag(tokens, corpus=\"orchid\", engine=\"perceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spell checking\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 PyThaiNLP's spell checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.spell import correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispelled_words = [\n",
    "    \"‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏ô\",\n",
    "    \"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥\",\n",
    "    \"‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏£‡∏≤‡∏ò‡∏¥‡∏õ‡∏î‡∏µ\",\n",
    "    \"‡∏™‡∏±‡∏õ‡∏õ‡∏∞‡∏£‡∏î\",\n",
    "    \"‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏∏\",\n",
    "    \"‡πÄ‡∏´‡∏ï‡∏Å‡∏≤‡∏£‡∏ì‡πå\",\n",
    "    \"‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏¥\",\n",
    "    \"‡∏ù‡∏±‡∏Å‡πÑ‡∏ù‡πà\",\n",
    "    \"‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ç‡∏°‡∏ô‡∏ï‡∏µ\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏ô -> ‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•\n",
      "\n",
      "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥ -> ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ\n",
      "\n",
      "‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏£‡∏≤‡∏ò‡∏¥‡∏õ‡∏î‡∏µ -> ‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô‡∏≤‡∏ò‡∏¥‡∏ö‡∏î‡∏µ\n",
      "\n",
      "‡∏™‡∏±‡∏õ‡∏õ‡∏∞‡∏£‡∏î -> ‡∏™‡∏±‡∏ö‡∏õ‡∏∞‡∏£‡∏î\n",
      "\n",
      "‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏∏ -> ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï\n",
      "\n",
      "‡πÄ‡∏´‡∏ï‡∏Å‡∏≤‡∏£‡∏ì‡πå -> ‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå\n",
      "\n",
      "‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏¥ -> ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï\n",
      "\n",
      "‡∏ù‡∏±‡∏Å‡πÑ‡∏ù‡πà -> ‡∏ù‡∏±‡∏Å‡πÉ‡∏ù‡πà\n",
      "\n",
      "‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ç‡∏°‡∏ô‡∏ï‡∏µ -> ‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in mispelled_words:\n",
    "    print(\"{} -> {}\".format(word, correct(word)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Try out:__ Put any mispelling words and correct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Thai digits and currency Conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.util import (\n",
    "    thai_digit_to_arabic_digit,\n",
    "    arabic_digit_to_thai_digit,\n",
    "    bahttext,\n",
    "    digit_to_text,\n",
    "    thaiword_to_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 31 ‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏° 2562 ‡πÄ‡∏ß‡∏•‡∏≤ 13:00 ‡∏ô. ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thai_digit_to_arabic_digit(\"‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡πì‡πë ‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏° ‡πí‡πï‡πñ‡πí ‡πÄ‡∏ß‡∏•‡∏≤ ‡πë‡πì:‡πê‡πê ‡∏ô. ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡πì‡πë ‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°‡πí‡πï‡πñ‡πí ‡πÄ‡∏ß‡∏•‡∏≤ ‡πë‡πì:‡πê‡πê ‡∏ô. ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_digit_to_thai_digit(\"‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 31 ‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°2562 ‡πÄ‡∏ß‡∏•‡∏≤ 13:00 ‡∏ô. ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏û‡∏±‡∏ô‡∏™‡∏≠‡∏á‡∏£‡πâ‡∏≠‡∏¢‡∏™‡∏≤‡∏°‡∏™‡∏¥‡∏ö‡∏™‡∏µ‡πà‡∏ö‡∏≤‡∏ó‡∏¢‡∏µ‡πà‡∏™‡∏¥‡∏ö‡∏™‡∏µ‡πà‡∏™‡∏ï‡∏≤‡∏á‡∏Ñ‡πå'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bahttext(1234.24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∏¢‡∏µ‡πà‡∏™‡∏¥‡∏ö‡πÄ‡∏≠‡πá‡∏î‡∏ö‡∏≤‡∏ó‡∏ñ‡πâ‡∏ß‡∏ô'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bahttext(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∏™‡∏≠‡∏á‡πÅ‡∏™‡∏ô‡∏™‡∏µ‡πà‡∏´‡∏°‡∏∑‡πà‡∏ô‡∏•‡πâ‡∏≤‡∏ô‡∏ö‡∏≤‡∏ó‡∏ñ‡πâ‡∏ß‡∏ô'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bahttext(240000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thaiword_to_num(\"‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏£‡πâ‡∏≠‡∏¢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∏™‡∏≤‡∏° ‡∏£‡πâ‡∏≠‡∏¢‡∏•‡πâ‡∏≤‡∏ô'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_to_text(\"‡πì ‡∏£‡πâ‡∏≠‡∏¢‡∏•‡πâ‡∏≤‡∏ô\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∏™‡∏≤‡∏°‡∏´‡∏ô‡∏∂‡πà‡∏á'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_to_text(\"‡πì‡πë\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thaiword_to_num(\"‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏û‡∏±‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000061"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thaiword_to_num(\"‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏•‡πâ‡∏≤‡∏ô‡∏´‡∏Å‡∏™‡∏¥‡∏ö‡πÄ‡∏≠‡πá‡∏î\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thaiword_to_num(\"‡∏û‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏ô\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Question 8 :__ Given a text representing an amont money, convert into number.\n",
    "\n",
    "```\n",
    "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤‡∏ã‡∏∂‡πà‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì ‡πë‡πí,‡πï‡πê‡πê ‡∏•‡πâ‡∏≤‡∏ô‡∏î‡∏≠‡∏•‡∏•‡∏≤‡∏£‡πå‡∏™‡∏´‡∏£‡∏±‡∏ê ‡πÉ‡∏´‡πâ‡∏ó‡∏ß‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô ‡πì ‡∏´‡∏°‡∏∑‡πà‡∏ô‡∏•‡πâ‡∏≤‡∏ô\n",
    "```\n",
    "->\n",
    "```\n",
    "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤‡∏ã‡∏∂‡πà‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 12,500 ‡∏•‡πâ‡∏≤‡∏ô‡∏î‡∏≠‡∏•‡∏•‡∏≤‡∏£‡πå‡∏™‡∏´‡∏£‡∏±‡∏ê ‡πÉ‡∏´‡πâ‡∏ó‡∏ß‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô 3 ‡∏´‡∏°‡∏∑‡πà‡∏ô‡∏•‡πâ‡∏≤‡∏ô\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_thai_digits = \"‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤‡∏ã‡∏∂‡πà‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì ‡πë‡πí,‡πï‡πê‡πê ‡∏•‡πâ‡∏≤‡∏ô‡∏î‡∏≠‡∏•‡∏•‡∏≤‡∏£‡πå‡∏™‡∏´‡∏£‡∏±‡∏ê ‡πÉ‡∏´‡πâ‡∏ó‡∏ß‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô ‡πì ‡∏´‡∏°‡∏∑‡πà‡∏ô‡∏•‡πâ‡∏≤‡∏ô\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(text):\n",
    "    splits = text.split(\" \")\n",
    "    \n",
    "    for index, split in enumerate(splits):\n",
    "        \n",
    "        if re.search(r\"[‡πê-‡πô]\", split):\n",
    "            print(\"\\nselcted split: \", split)\n",
    "            ## Modify the following line to convert from thai digits to arabic\n",
    "\n",
    "            splits[index] = split\n",
    "            \n",
    "            ##--------------------- ##\n",
    "            print(\"convert to: \", splits[index])\n",
    "    \n",
    "    return \" \".join(splits)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "selcted split:  ‡πë‡πí,‡πï‡πê‡πê\n",
      "convert to:  ‡πë‡πí,‡πï‡πê‡πê\n",
      "\n",
      "selcted split:  ‡πì\n",
      "convert to:  ‡πì\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤‡∏ã‡∏∂‡πà‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì ‡πë‡πí,‡πï‡πê‡πê ‡∏•‡πâ‡∏≤‡∏ô‡∏î‡∏≠‡∏•‡∏•‡∏≤‡∏£‡πå‡∏™‡∏´‡∏£‡∏±‡∏ê ‡πÉ‡∏´‡πâ‡∏ó‡∏ß‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô ‡πì ‡∏´‡∏°‡∏∑‡πà‡∏ô‡∏•‡πâ‡∏≤‡∏ô'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert(text_with_thai_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test:__ Given a list of sentences, please return only Thai sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "selcted split:  ‡πë‡πí,‡πï‡πê‡πê\n",
      "convert to:  ‡πë‡πí,‡πï‡πê‡πê\n",
      "\n",
      "selcted split:  ‡πì\n",
      "convert to:  ‡πì\n",
      "‚ùå Test failed. üò≠\n",
      "The actual results: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤‡∏ã‡∏∂‡πà‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì ‡πë‡πí,‡πï‡πê‡πê ‡∏•‡πâ‡∏≤‡∏ô‡∏î‡∏≠‡∏•‡∏•‡∏≤‡∏£‡πå‡∏™‡∏´‡∏£‡∏±‡∏ê ‡πÉ‡∏´‡πâ‡∏ó‡∏ß‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô ‡πì ‡∏´‡∏°‡∏∑‡πà‡∏ô‡∏•‡πâ‡∏≤‡∏ô\n"
     ]
    }
   ],
   "source": [
    "def test_convert_thai_digits(convert):\n",
    "    expect = \"‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤‡∏ã‡∏∂‡πà‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 12,500 ‡∏•‡πâ‡∏≤‡∏ô‡∏î‡∏≠‡∏•‡∏•‡∏≤‡∏£‡πå‡∏™‡∏´‡∏£‡∏±‡∏ê ‡πÉ‡∏´‡πâ‡∏ó‡∏ß‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô 3 ‡∏´‡∏°‡∏∑‡πà‡∏ô‡∏•‡πâ‡∏≤‡∏ô\"\n",
    "    actual = convert(text_with_thai_digits)\n",
    "\n",
    "    if actual == expect:\n",
    "        print(\"‚úÖ Test succeed. üòÅ\")\n",
    "    else:\n",
    "        print(\"‚ùå Test failed. üò≠\")\n",
    "        print(\"The actual results:\", actual)\n",
    "\n",
    "test_convert_thai_digits(convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(text):\n",
    "    splits = text.split(\" \")\n",
    "    \n",
    "    for index, split in enumerate(splits):\n",
    "        \n",
    "        if re.search(r\"[‡πê-‡πô]\", split):\n",
    "            print(\"\\nselcted split: \", split)\n",
    "            ## Write the code to convert\n",
    "\n",
    "            splits[index] = thai_digit_to_arabic_digit(split)\n",
    "            \n",
    "            \n",
    "            print(\"convert to: \", splits[index])\n",
    "\n",
    "    return \" \".join(splits)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "selcted split:  ‡πë‡πí,‡πï‡πê‡πê\n",
      "convert to:  12,500\n",
      "\n",
      "selcted split:  ‡πì\n",
      "convert to:  3\n",
      "‚úÖ Test succeed. üòÅ\n"
     ]
    }
   ],
   "source": [
    "test_convert_thai_digits(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Thai Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.util import countthai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Count percentage of Thai chacters in a text.\n",
    "\n",
    "```python\n",
    " countthai(text:str) -> percentage:float\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countthai(\"Hello world.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countthai(\"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡∏â‡∏±‡∏ô‡∏ä‡∏≠‡∏ö‡∏ô‡∏±‡πà‡∏á‡∏£‡∏ñ‡πÑ‡∏ü\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.15384615384615"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countthai(\"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ Jane Doe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Question 9:__ Given a list of sentences, please return only Thai sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_th_sentences = [\n",
    "    \"‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÑ‡∏î‡πâ‡∏•‡πà‡∏∞?\",\n",
    "    \"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ... ...‡∏î‡∏¥‡πä‡∏Å\",\n",
    "    \"# Just to get a glimpse beyond this illusion #\",\n",
    "    \"# I was soaring ever higher #\",\n",
    "    \"# but I flew too high #\",\n",
    "    \"    ‡πÉ‡∏ä‡πà\",\n",
    "    \"‡πÉ‡∏ä‡πà ‡πÄ‡∏Ç‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏ü‡∏£‡∏á‡∏Ñ‡πå ‡πÅ‡∏•‡∏∞ ‡πÅ‡∏Ñ‡∏™ ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏ß‡∏î‡πÄ‡∏´‡∏•‡πâ‡∏≤‡πÄ‡∏Ç‡∏≤ ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Å‡∏£‡∏∞‡πÄ‡∏õ‡πã‡∏≤\",\n",
    "    \"‡∏°‡∏±‡∏ô‡∏Å‡πá‡∏î‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≤‡∏ß‡∏•‡∏µ‡∏¢‡πå ‡∏°‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ù‡∏±‡πà‡∏á‡πÄ‡∏£‡∏≤‡∏ñ‡∏π‡∏Å‡∏°‡∏±‡πâ‡∏¢?\",\n",
    "    \"‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏£‡∏≤‡∏ß‡∏•‡∏µ‡∏¢‡πå ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏•‡∏¢\",\n",
    "    \"‡πÄ‡∏ä‡∏¥‡∏ç‡∏ô‡∏±‡πà‡∏á\",\n",
    "    \"== sync, corrected by elderman ==\",\n",
    "    \"# though my eyes could see, I still was a blind man #\",\n",
    "    \"# though my mind could think, I still was a madman #\",\n",
    "    \"‡πÑ‡∏î‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ ‡∏û‡∏ß‡∏Å‡∏°‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏≤‡∏ï‡∏≤‡∏°‡∏•‡πà‡∏≤‡πÄ‡∏Ç‡∏≤\",\n",
    "    \"# I hear the voices when I'm dreaming #\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_thai_sentence(sentence):\n",
    "\n",
    "    ## Write down the code, to return value True if the sentence is in Thai language\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_sentences = [\n",
    "    \"‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÑ‡∏î‡πâ‡∏•‡πà‡∏∞?\",\n",
    "    \"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ... ...‡∏î‡∏¥‡πä‡∏Å\",\n",
    "    \"    ‡πÉ‡∏ä‡πà\",\n",
    "    \"‡πÉ‡∏ä‡πà ‡πÄ‡∏Ç‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏ü‡∏£‡∏á‡∏Ñ‡πå ‡πÅ‡∏•‡∏∞ ‡πÅ‡∏Ñ‡∏™ ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏ß‡∏î‡πÄ‡∏´‡∏•‡πâ‡∏≤‡πÄ‡∏Ç‡∏≤ ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Å‡∏£‡∏∞‡πÄ‡∏õ‡πã‡∏≤\",\n",
    "    \"‡∏°‡∏±‡∏ô‡∏Å‡πá‡∏î‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≤‡∏ß‡∏•‡∏µ‡∏¢‡πå ‡∏°‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ù‡∏±‡πà‡∏á‡πÄ‡∏£‡∏≤‡∏ñ‡∏π‡∏Å‡∏°‡∏±‡πâ‡∏¢?\",\n",
    "    \"‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏£‡∏≤‡∏ß‡∏•‡∏µ‡∏¢‡πå ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏•‡∏¢\",\n",
    "    \"‡πÄ‡∏ä‡∏¥‡∏ç‡∏ô‡∏±‡πà‡∏á\",\n",
    "    \"‡πÑ‡∏î‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ ‡∏û‡∏ß‡∏Å‡∏°‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏≤‡∏ï‡∏≤‡∏°‡∏•‡πà‡∏≤‡πÄ‡∏Ç‡∏≤\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Test failed. üò≠\n",
      "The actual results: []\n",
      "\n",
      "The expected sentences to be returned:\n",
      "\n",
      "1 ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÑ‡∏î‡πâ‡∏•‡πà‡∏∞?\n",
      "2 ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ... ...‡∏î‡∏¥‡πä‡∏Å\n",
      "3     ‡πÉ‡∏ä‡πà\n",
      "4 ‡πÉ‡∏ä‡πà ‡πÄ‡∏Ç‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏ü‡∏£‡∏á‡∏Ñ‡πå ‡πÅ‡∏•‡∏∞ ‡πÅ‡∏Ñ‡∏™ ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏ß‡∏î‡πÄ‡∏´‡∏•‡πâ‡∏≤‡πÄ‡∏Ç‡∏≤ ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Å‡∏£‡∏∞‡πÄ‡∏õ‡πã‡∏≤\n",
      "5 ‡∏°‡∏±‡∏ô‡∏Å‡πá‡∏î‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≤‡∏ß‡∏•‡∏µ‡∏¢‡πå ‡∏°‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ù‡∏±‡πà‡∏á‡πÄ‡∏£‡∏≤‡∏ñ‡∏π‡∏Å‡∏°‡∏±‡πâ‡∏¢?\n",
      "6 ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏£‡∏≤‡∏ß‡∏•‡∏µ‡∏¢‡πå ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏•‡∏¢\n",
      "7 ‡πÄ‡∏ä‡∏¥‡∏ç‡∏ô‡∏±‡πà‡∏á\n",
      "8 ‡πÑ‡∏î‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ ‡∏û‡∏ß‡∏Å‡∏°‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏≤‡∏ï‡∏≤‡∏°‡∏•‡πà‡∏≤‡πÄ‡∏Ç‡∏≤\n"
     ]
    }
   ],
   "source": [
    "actual = list(filter(test_thai_sentence, en_th_sentences))\n",
    "expect = th_sentences\n",
    "\n",
    "if actual == expect:\n",
    "    print(\"‚úÖ Test succeed. üòÅ\")\n",
    "else:\n",
    "    print(\"‚ùå Test failed. üò≠\")\n",
    "    print(\"The actual results:\", actual)\n",
    "    print(\"\\nThe expected sentences to be returned:\\n\")\n",
    "    for i, sentence in enumerate(th_sentences):\n",
    "        print(i+1, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_thai_sentence(sentence):\n",
    "\n",
    "    ## Write down the code, to return value True if the sentence is in Thai language\n",
    "    \n",
    "    if countthai(sentence)  >= 90.0:\n",
    "        return True\n",
    "    \n",
    "    ##\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test succeed. üòÅ\n"
     ]
    }
   ],
   "source": [
    "actual = list(filter(test_thai_sentence, en_th_sentences))\n",
    "expect = th_sentences\n",
    "\n",
    "if actual == expect:\n",
    "    print(\"‚úÖ Test succeed. üòÅ\")\n",
    "else:\n",
    "    print(\"‚ùå Test failed. üò≠\")\n",
    "    print(\"The actual results:\", actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data and time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function uses Thai names and Thai Buddhist Era for these directives:\n",
    "\n",
    "- __%a__ - abbreviated weekday name (e.g. ‚Äú‡∏à‚Äù, ‚Äú‡∏≠‚Äù, ‚Äú‡∏û‚Äù, ‚Äú‡∏û‡∏§‚Äù, ‚Äú‡∏®‚Äù, ‚Äú‡∏™‚Äù, ‚Äú‡∏≠‡∏≤‚Äù)\n",
    "\n",
    "- __%A__ - full weekday name (e.g.‚Äú‡∏ß‡∏±‡∏ô‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå‚Äù, ‚Äú‡∏ß‡∏±‡∏ô‡∏≠‡∏±‡∏á‡∏Ñ‡∏≤‡∏£‚Äù, ‚Äú‡∏ß‡∏±‡∏ô‡πÄ‡∏™‡∏≤‡∏£‡πå‚Äù, ‚Äú‡∏ß‡∏±‡∏ô‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå‚Äù)\n",
    "\n",
    "- __%b__ - abbreviated month name (e.g.‚Äú‡∏°.‡∏Ñ.‚Äù,‚Äù‡∏Å.‡∏û.‚Äù,‚Äù‡∏°‡∏µ.‡∏Ñ.‚Äù,‚Äù‡πÄ‡∏°.‡∏¢.‚Äù,‚Äù‡∏û.‡∏Ñ.‚Äù,‚Äù‡∏°‡∏¥.‡∏¢.‚Äù, ‚Äú‡∏ò.‡∏Ñ.‚Äù)\n",
    "\n",
    "- __%B__ - full month name (e.g. ‚Äú‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°‚Äù, ‚Äú‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå‚Äù, ‚Äú‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô‚Äù, ‚Äú‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°‚Äù,)\n",
    "\n",
    "- __%y__ - year without century (e.g. ‚Äú56‚Äù, ‚Äú10‚Äù)\n",
    "\n",
    "- __%Y__ - year with century (e.g. ‚Äú2556‚Äù, ‚Äú2410‚Äù)\n",
    "\n",
    "- __%c__ - date and time representation (e.g. ‚Äú‡∏û 6 ‡∏ï.‡∏Ñ. 01:40:00 2519‚Äù)\n",
    "\n",
    "- __%v__ - short date representation (e.g. ‚Äù 6-‡∏°.‡∏Ñ.-2562‚Äù, ‚Äú27-‡∏Å.‡∏û.-2555‚Äù)\n",
    "\n",
    "- __%d__ - day (e.g. \"01\", \"07\", 10\", \"31\")\n",
    "\n",
    "- __%-d__ - day with no zero padding (e.g. \"1\", \"7\",10\", \"31\")\n",
    " \n",
    "- __%H__  - hour (e.g. \"01\", \"06\", \"23\")\n",
    "\n",
    "- __%-H__ - hour with no zero padding (e.g. \"1\", \"6\", \"23\"))\n",
    "\n",
    "- __%M__  - minute (e.g. \"1\", \"2\", \"11\", \"12\")\n",
    "\n",
    "- __%S__  - second (e.g. \"1\", \"2\", \"11\", \"12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pythainlp.util import thai_strftime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô 2562'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the current date in Thai format\n",
    "\n",
    "thai_strftime(datetime.now(), \"%d %B %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Question 10:__ Given a date time object, return the datetime string in the following format\n",
    "\n",
    "\n",
    "```\n",
    "‡∏ß‡∏±‡∏ô‡∏®‡∏∏‡∏Å‡∏£‡πå ‡∏ó‡∏µ‡πà 1 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô ‡∏õ‡∏µ ‡∏û.‡∏®. 2562 ‡πÄ‡∏ß‡∏•‡∏≤ 11 ‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤ 30 ‡∏ô‡∏≤‡∏ó‡∏µ 10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_object_workshop_day = datetime(year=2019, month=11, day=1,hour=11,minute=30,second=10)\n",
    "\n",
    "def print_datetime_thai(datetime_object):\n",
    "    # Write down the format string.\n",
    "    fmt = \"\"\n",
    "    return thai_strftime(datetime_object, fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Test failed. üò≠\n",
      "\n",
      "Your result    : \n",
      "\n",
      "Expected result: ‡∏ß‡∏±‡∏ô‡∏®‡∏∏‡∏Å‡∏£‡πå ‡∏ó‡∏µ‡πà 1 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô ‡∏û.‡∏®. 2562 ‡πÄ‡∏ß‡∏•‡∏≤ 11 ‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤ 30 ‡∏ô‡∏≤‡∏ó‡∏µ 10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\n"
     ]
    }
   ],
   "source": [
    "def test_print_datetime_thai(fn):\n",
    "    expect = \"‡∏ß‡∏±‡∏ô‡∏®‡∏∏‡∏Å‡∏£‡πå ‡∏ó‡∏µ‡πà 1 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô ‡∏û.‡∏®. 2562 ‡πÄ‡∏ß‡∏•‡∏≤ 11 ‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤ 30 ‡∏ô‡∏≤‡∏ó‡∏µ 10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\"\n",
    "    actual = fn(datetime_object_workshop_day)\n",
    "\n",
    "    if actual == expect:\n",
    "        print(\"‚úÖ Test succeed. üòÅ\\n\")\n",
    "        print(\"Your Result:\",expect)\n",
    "    else:\n",
    "        print(\"‚ùå Test failed. üò≠\")\n",
    "        print(\"\\nYour result    :\", actual)\n",
    "        print(\"\\nExpected result:\", expect)\n",
    "\n",
    "test_print_datetime_thai(print_datetime_thai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test succeed. üòÅ\n",
      "\n",
      "Your Result: ‡∏ß‡∏±‡∏ô‡∏®‡∏∏‡∏Å‡∏£‡πå ‡∏ó‡∏µ‡πà 1 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô ‡∏û.‡∏®. 2562 ‡πÄ‡∏ß‡∏•‡∏≤ 11 ‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤ 30 ‡∏ô‡∏≤‡∏ó‡∏µ 10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\n"
     ]
    }
   ],
   "source": [
    "def print_datetime_thai(datetime_object):\n",
    "    # Write down the format string.\n",
    "    fmt = \"%A ‡∏ó‡∏µ‡πà %-d %B ‡∏û.‡∏®. %Y ‡πÄ‡∏ß‡∏•‡∏≤ %H ‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤ %M ‡∏ô‡∏≤‡∏ó‡∏µ %S ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\"\n",
    "    return thai_strftime(datetime_object, fmt)\n",
    "\n",
    "test_print_datetime_thai(print_datetime_thai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Without Thai strftime__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Friday ‡∏ó‡∏µ‡πà 1 November ‡∏û.‡∏®. 2019 ‡πÄ‡∏ß‡∏•‡∏≤ 11 ‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤ 30 ‡∏ô‡∏≤‡∏ó‡∏µ 10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_object_workshop_day.strftime(\"%A ‡∏ó‡∏µ‡πà %-d %B ‡∏û.‡∏®. %Y ‡πÄ‡∏ß‡∏•‡∏≤ %H ‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤ %M ‡∏ô‡∏≤‡∏ó‡∏µ %S ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
