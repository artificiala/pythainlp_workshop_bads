{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Notebook 3: Text Clustering on NECTEC's Thai QA Dataset\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, we will cluster type of Thai questions by its semantic similarity.\n",
    "\n",
    "\n",
    "We will use data from Thai QA Dataset (https://www.nectec.or.th/corpus/index.php?league=tp5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user -q --upgrade --pre pythainlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header\n",
    "\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from pythainlp.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Question Answeing Dataset\n",
    "\n",
    "\n",
    "__Thai Question Answering Dataset from Wikipedia created by NECTEC__\n",
    "\n",
    "Reference: http://copycatch.in.th/thai-qa-task.html\n",
    "\n",
    "ชุดข้อมูลนี้ประกอบด้วยกลุ่มคู่คำถามและคำตอบที่ถูกสร้างจากผู้ใช้ทั่วไปและเป็นกลุ่มคู่คำถามและคำตอบที่มีเนื้อหาหลากหลาย เช่น ด้านวิทยาศาสตร์ การท่องเที่ยว กีฬา และอื่น ๆ นอกจากนี้คำถามที่อยู่ในกลุ่มนี้เป็นคำถามง่ายและยากผสมกัน \n",
    "\n",
    "คำถามที่เป็นข้อเท็จจริง (Factoid question answering) เป็นคำถามที่ง่ายๆ ถามเกี่ยวกับข้อเท็จจริงและเป็นคำถามที่มีคำตอบจริง โดยจะมีคำแสดงคำถามได้แก่ ใคร อะไร ไหน ที่ไหน เมื่อไร ใด กี่ เท่าไร ยกตัวอย่างเช่น\n",
    "\n",
    "-----\n",
    "\n",
    "__คำถาม:__ นายกรัฐมนตรีคนที่ 7 ของประเทศไทยคือใคร \n",
    "\n",
    "__คำตอบ:__ ปรีดี พนมยงค์\n",
    "\n",
    "-----\n",
    "\n",
    "__คำถาม:__ กีฬาประจำชาติแห่งแดนอาทิตย์อุทัยที่มีประวัติยาวนานคือกีฬาอะไร \n",
    "\n",
    "__คำตอบ:__ ซูโม่\n",
    "\n",
    "-----\n",
    "\n",
    "__คำถาม:__ ออสเตรเลียเป็นประเทศร่ำรวยเป็นอันดับที่เท่าไรของโลก\n",
    "\n",
    "__คำตอบ:__ 12\n",
    "\n",
    "-----\n",
    "\n",
    "__คำถาม:__ พระกระโดดกำแพงเป็นอาหารประจำชาติไหนในทวีปเอเชีย \n",
    "\n",
    "__คำตอบ:__ จีน\n",
    "\n",
    "-----\n",
    "\n",
    "__คำถาม:__ รายงานความสุขโลกเป็นดัชนีวัดความสุขเผยแพร่โดยใคร \n",
    "\n",
    "__คำตอบ:__ สหประชาชาติ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "\n",
    "dataset = None\n",
    "\n",
    "with open(\"../data/thai_qa/ThaiQACorpus-DevelopmentDataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Number of questions:\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qa in dataset[:10]:\n",
    "    print(qa)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question words \n",
    "\n",
    "__Wh-question:__ `What, When, Where, Why, Which, How`\n",
    "\n",
    "- In Thai questions, it may occur at the end of sentence, or at the begining.\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "\n",
    "\n",
    "- \"__ใครคือ__ผู้บริหารสำนักงานส่งเสริมเศรษฐกิดิจิทัล\" -> Who is the director of Digital Economy Promotion Agency?\n",
    "- \"แม่ของดอนัลด์ ทรัมป์__คือใคร__\" -> Who is the mother of Donald Trump?\n",
    "- \"บารัค โอบามา มีเชื่อชาติ__อะไร__\" -> What is the race of Barack Obama?\n",
    "- \"ดอนัลด์ ทรัมป์ เกิด__เมื่อใด__\" -> When was Donald Trump born?\n",
    "- \"ทะเลเดดซี __ตั้งอยู่ที่ใด__\" -> Where is the Dead Sea located?\n",
    "- \"คุณอาศัยอยู่ที่__ประเทศใด__\" -> Which country do  you live in?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Look at the last 3 tokens and fist 3 tokens__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()\n",
    "for qa in dataset[1000:1025]:\n",
    "    question = qa[\"question\"]\n",
    "    print(\"Question:\", question)\n",
    "    toks = word_tokenize(question, keep_whitespace=False)\n",
    "    print(\"Tokens:\", \"|\".join(toks))\n",
    "    print(\"First 3 Tokens:\", \"|\".join(toks[:3]))\n",
    "    print(\"Last 3 Tokens:\", \"|\".join(toks[-3:]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentence Vectorization\n",
    "Use Sentence Vectorizer to represent a sentence as a 300 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from pythainlp.word_vector import similarity, sentence_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Calculate Cosine similarility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](images/cosine_sim.png)\n",
    "image from https://cmry.github.io/notes/euclidean-v-cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caluculate_consine_similarity(sentence_a, sentence_b):\n",
    "    vec1, vec2 = sentence_vectorizer(sentence_a),sentence_vectorizer(sentence_b)\n",
    "    return cosine_similarity(vec1, vec2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caluculate_consine_similarity(\"ทานอาหาร\", \"กินข้าว\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caluculate_consine_similarity(\"กระทรวงโฆษณาแถลงข่าวและโฆษณาชวนเชื่อของนาซีเยอรมนี ก่อตั้งขึ้นในปี ค.ศ. ใด\",\n",
    "                              \"กระทรวงดิจิทัลเพื่อเศรษฐกิจและสังคม ในประเทศไทย ก่อตั้งในปี พ.ศ. ใด\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caluculate_consine_similarity(\"กระทรวงโฆษณาแถลงข่าวและโฆษณาชวนเชื่อของนาซีเยอรมนี ก่อตั้งขึ้นในปี ค.ศ. ใด\",\n",
    "                              \"สุนัขตัวแรกรับบทเป็นเบนจี้ในภาพยนตร์เรื่อง Benji มีชื่อว่าอะไร\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Try out:__ Add you own sentences or words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caluculate_consine_similarity(\" \", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Vectorize all questions in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_feat = np.array([ sentence_vectorizer(qa[\"question\"]) for qa in dataset ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_feat = question_feat.reshape(len(dataset), -1)\n",
    "question_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Save as .tsv file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('./qa/vector.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('./qa/meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, qa in enumerate(dataset):\n",
    "    question = qa[\"question\"]\n",
    "    vector = question_feat[index]\n",
    "    out_m.write(question + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vector]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize sentence embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### __1. Go to: http://projector.tensorflow.org/__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### __2. Click \"Load\" button on the left panel__\n",
    "\n",
    "![title2](images/projector_left_panel.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### __3. Then, Upload `vector.tsv`, and `meta.tsv`__\n",
    "\n",
    "![title3](images/projector_load_data.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### __4. Select T-SNE, the screen will show the embedding vector for each sentence in 3D space.__\n",
    "\n",
    "![title1](images/projector_main.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Clustering with K-Mean Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import k_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_texts = []\n",
    "for qa in dataset:\n",
    "    tokens = word_tokenize(qa[\"question\"], keep_whitespace=False)\n",
    "    question_text = ''.join(tokens)\n",
    "    question_texts.append(question_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_feats = np.array([ sentence_vectorizer(question_text) for question_text in question_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_feats = question_feats.reshape(len(dataset), -1)\n",
    "question_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clusters(n_clusters):\n",
    "    centroids, labels, _ = k_means(question_feats, n_clusters=n_clusters)\n",
    "    return centroids, labels\n",
    "\n",
    "def explore_cluster(n_clusters, centroids, labels, n_qas=10):\n",
    "    clusters = { i: [] for i in range(0, N_CLUSTERS, 1) }\n",
    "    for index, label in enumerate(labels):\n",
    "        clusters[label].append( dataset[index][\"question\"])\n",
    "    \n",
    "    for cluster_id, members in clusters.items():\n",
    "\n",
    "        print(\"Cluster ID:\", cluster_id)\n",
    "        print(\"Number of questions in this cluster:\", len(members))\n",
    "        for i, member in enumerate(members[100:100+n_qas]):\n",
    "            print(\"\")\n",
    "            print((i+1), \"\", member)\n",
    "        print(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids, labels = build_clusters(7)\n",
    "explore_cluster(7, centroids, labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Try out:__ Cluster with the different number of clusters (N_CLUSTERS) and see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the number of clusters\n",
    "N_CLUSTERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "centroids, labels = build_clusters(N_CLUSTERS)\n",
    "explore_cluster(N_CLUSTERS, centroids, labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
